/**************************************************************
  @author: Jin Kyu Kim (jinkyuk@cs.cmu.edu)

***************************************************************/
#pragma once 

#include <iostream>
#include <stdio.h>
#include <map>
#include <unordered_map>
#include <vector>
#include <iostream>
#include <math.h>
#include <stdlib.h>
#include <gflags/gflags.h>
#include <glog/logging.h>
#include <assert.h>
#include <zmq.hpp>


//class sharedctx;
//class context; // from zmq/zmq-common.hpp 

#include <strads/include/coordinator.hpp>
#include <strads/include/worker.hpp>
#include <strads/include/scheduler.hpp>

#include <strads/include/strads-pi.hpp>
#include <strads/include/sysparam.hpp>
#include <strads/util/utility.hpp>
#include <strads/netdriver/comm.hpp>
#include <strads/netdriver/zmq/zmq-common.hpp>
#include <strads/ds/dshard.hpp>

sharedctx *strads_init(int argc, char **argv);

void *process_common_system_cmd(sharedctx *ctx, mbuffer *mbuf, context *recv_ctx, context *send_ctx, bool *tflag);
void mcopy_broadcast_to_workers(sharedctx *ctx, void *tmp, int len);
void mcopy_broadcast_to_workers_objectcalc(sharedctx *ctx, void *tmp, int len);

class task_assignment{
public:
  std::map<int, range *>schmach_tmap;
  std::map<int, range *>schthrd_tmap;
};

typedef struct {
  int64_t size;
  int64_t phaseno;
  uobj_type  type; // user defined type that allows user to parse the data entry 
  void *data;
}staleinfo;


//#define IHWM (10)
#define IHWM (MAX_ZMQ_HWM - 5)

// This class does not have declaration of destructor 
//  since it will not be deleted as far as the program runs 
class sharedctx {

public:
  sharedctx(const int r, sysparam *sp):
    rank(r), m_mrole(mrole_unknown), m_sport_lock(PTHREAD_MUTEX_INITIALIZER), m_rport_lock(PTHREAD_MUTEX_INITIALIZER), 
    m_sport_flag(false), m_rport_flag(false), ring_sport(NULL), ring_rport(NULL), m_fsport_lock(PTHREAD_MUTEX_INITIALIZER), 
    m_frport_lock(PTHREAD_MUTEX_INITIALIZER), m_fsport_flag(false), m_frport_flag(false), ring_fsport(NULL), ring_frport(NULL), 
    m_starsport_lock(PTHREAD_MUTEX_INITIALIZER), m_starrport_lock(PTHREAD_MUTEX_INITIALIZER),  m_starsend(PTHREAD_MUTEX_INITIALIZER),
    m_starrecv(PTHREAD_MUTEX_INITIALIZER), m_sp(sp), m_idvalp_buf(NULL), m_freethrds_lock(PTHREAD_MUTEX_INITIALIZER), 
    m_freethrds(0), m_schedctx(NULL), m_coordctx(NULL), m_workerctx(NULL), m_ringtoken_send_lock(PTHREAD_MUTEX_INITIALIZER), 
    m_ringtoken_recv_lock(PTHREAD_MUTEX_INITIALIZER) {


    m_scheduler_mid = -1; 
    m_worker_mid = -1;    
    m_coordinator_mid = -1; 

    m_zmqcontext=NULL;

    m_mrole = mrole_unknown;


    m_ringtoken_send = IHWM;
    m_ringtoken_recv = 0;

    //    m_mbuffers = (mbuffer **)calloc(sizeof(mbuffer *), MAX_MACHINES*10);
  };

  sharedctx():
    rank(-1){};

  std::map<int, mnode *> nodes; // machine nodes
  std::map<int, mlink *> links; // node graph links
  std::map<int, mlink *> rlinks; // node graph links
  int rank;                     // machine id
  mach_role m_mrole;

  void *(*machine_func)(void *); 

  void set_sport_flag(bool flag){
    pthread_mutex_lock(&m_sport_lock);
    m_sport_flag = flag;
    pthread_mutex_unlock(&m_sport_lock);
  }

  void set_rport_flag(bool flag){
    pthread_mutex_lock(&m_rport_lock);
    m_rport_flag = flag;
    pthread_mutex_unlock(&m_rport_lock);
  }

  bool get_rport_flag(void){
    bool ret;
    pthread_mutex_lock(&m_rport_lock);
    ret = m_rport_flag;
    pthread_mutex_unlock(&m_rport_lock);
    return ret;
  }

  bool get_sport_flag(void){
    bool ret;
    pthread_mutex_lock(&m_sport_lock);
    ret = m_sport_flag;
    pthread_mutex_unlock(&m_sport_lock);
    return ret;
  }

  void set_rport(_ringport *rport){
    pthread_mutex_lock(&m_rport_lock);
    ring_rport = rport;
    pthread_mutex_unlock(&m_rport_lock);   
  }

  void set_sport(_ringport *sport){
    pthread_mutex_lock(&m_sport_lock);
    ring_sport = sport;
    pthread_mutex_unlock(&m_sport_lock);   
  }

  // fast ring stuff ....
  void set_fsport_flag(bool flag){
    pthread_mutex_lock(&m_fsport_lock);
    m_fsport_flag = flag;
    pthread_mutex_unlock(&m_fsport_lock);
  }

  void set_frport_flag(bool flag){
    pthread_mutex_lock(&m_frport_lock);
    m_frport_flag = flag;
    pthread_mutex_unlock(&m_frport_lock);
  }

  bool get_frport_flag(void){
    bool ret;
    pthread_mutex_lock(&m_frport_lock);
    ret = m_frport_flag;
    pthread_mutex_unlock(&m_frport_lock);
    return ret;
  }

  bool get_fsport_flag(void){
    bool ret;
    pthread_mutex_lock(&m_fsport_lock);
    ret = m_fsport_flag;
    pthread_mutex_unlock(&m_fsport_lock);
    return ret;
  }

  void set_frport(_ringport *rport){
    pthread_mutex_lock(&m_frport_lock);
    ring_frport = rport;
    pthread_mutex_unlock(&m_frport_lock);   
  }

  void set_fsport(_ringport *sport){
    pthread_mutex_lock(&m_fsport_lock);
    ring_fsport = sport;
    pthread_mutex_unlock(&m_fsport_lock);   
  }

  void insert_star_recvport(uint16_t port, _ringport *ctx){
    int r = pthread_mutex_lock(&m_starrecv);
    checkResults("[insert star recvport] get lock failed", r);
    star_recvportmap.insert(std::pair<uint16_t, _ringport *>(port, ctx));
    r = pthread_mutex_unlock(&m_starrecv);   
    checkResults("[insert star recvport] release lock failed", r);
  }

  void insert_star_sendport(uint16_t port, _ringport *ctx){
    int r = pthread_mutex_lock(&m_starsend);
    checkResults("[insert star sendport] release lock failed", r);
    star_sendportmap.insert(std::pair<uint16_t, _ringport *>(port, ctx));
    pthread_mutex_unlock(&m_starsend);   
    checkResults("[insert star sendport] release lock failed", r);
  }

  unsigned int get_size_recvport(void){
    unsigned int ret;
    int r = pthread_mutex_lock(&m_starrecv);
    checkResults("[insert star recvport] get lock failed", r);
    ret = star_recvportmap.size();
    r = pthread_mutex_unlock(&m_starrecv);   
    checkResults("[insert star recvport] release lock failed", r);
    return ret;
  }

  unsigned int get_size_sendport(void){
    unsigned int ret;
    int r = pthread_mutex_lock(&m_starsend);
    checkResults("[insert star sendport] get lock failed", r);
    ret = star_sendportmap.size();
    r = pthread_mutex_unlock(&m_starsend);   
    checkResults("[insert star sendport] release lock failed", r);
    return ret;
  }

  void get_lock_sendportmap(void){
    int r = pthread_mutex_lock(&m_starsend);
    checkResults("[get lock on sendport] get lock failed", r);
  }

  void release_lock_sendportmap(void){
    int r = pthread_mutex_unlock(&m_starsend);
    checkResults("[release lock sendport] unlock failed", r);
  }

  void get_lock_recvportmap(void){
    int r = pthread_mutex_lock(&m_starrecv);
    checkResults("[get lock on recvport] get lock failed", r);
  }

  void release_lock_recvportmap(void){
    int r = pthread_mutex_unlock(&m_starrecv);
    checkResults("[release lock on recv port] unlock failed", r);
  }

  void bind_params(sysparam *param){
    assert(param);
    m_sp = param;
  }

  void make_scheduling_taskpartition(int64_t modelsize, int schedmach, int thrd_permach){

    int parts = schedmach*thrd_permach;   
    int64_t share = modelsize/parts;
    int64_t remain = modelsize % parts;
    int64_t start, end, myshare;

    //  give global assignment scheme per thread 
    for(int i=0; i < parts; i++){
      if(i==0){
	start = 0;
      }else{
	start = m_tmap.schthrd_tmap[i-1]->end + 1;
      }
      if(i < remain){
	myshare = share+1;
      }else{
	myshare = share;
      }
      end = start + myshare -1;
      if(end >= modelsize){
	end = modelsize -1;
      }
      range *tmp = new range;
      tmp->start = start;
      tmp->end = end;
      m_tmap.schthrd_tmap.insert(std::pair<int, range *>(i, tmp));			      
    }
    
    // give global assignment per mach 
    for(int i=0; i < schedmach; i++){
      int start_thrd = i*thrd_permach;
      int end_thrd = start_thrd + thrd_permach - 1;
      range *tmp = new range;
      tmp->start = m_tmap.schthrd_tmap[start_thrd]->start;
      tmp->end = m_tmap.schthrd_tmap[end_thrd]->end;
      m_tmap.schmach_tmap.insert(std::pair<int, range *>(i, tmp));
    }

    if(rank == 0){
      for(auto p : m_tmap.schthrd_tmap){
	strads_msg(ERR, "[scheduling task partitioning] scheduler thread(%d) start(%ld) end(%ld)\n", 
		   p.first, p.second->start, p.second->end); 
      }

      for(auto p : m_tmap.schmach_tmap){
	strads_msg(ERR, "[scheduling task partitioning] scheduler mach(%d) start(%ld) end(%ld)\n", 
		   p.first, p.second->start, p.second->end); 
      }
    }
  }

  mach_role find_role(int mpi_size){

    mach_role mrole = mrole_unknown;
    int sched_machines = m_sp->m_schedulers;      
    int first_schedmach = mpi_size - sched_machines - 1;
    int first_coordinatormach = mpi_size - 1; 


    m_sched_machines = m_sp->m_schedulers;      
    m_first_schedmach = mpi_size - m_sched_machines - 1;
    m_worker_machines = m_first_schedmach;
    m_first_workermach = 0;
    m_first_coordinatormach = mpi_size - 1; 


    if(rank == 0)
      strads_msg(ERR, "@@@@@@@@ FIND ROLE : Rank(%d) schedmach(%d) schedmach(%d) coordinatemach (%d)\n",
		 rank, sched_machines, first_schedmach, first_coordinatormach);

    if(rank == first_coordinatormach){
      mrole = mrole_coordinator; 
    }else if(rank < first_schedmach){       
      mrole = mrole_worker;
    }else if(rank >= first_schedmach && rank < first_coordinatormach){
      mrole = mrole_scheduler;
    }
    assert(mrole != mrole_unknown);

    strads_msg(ERR, "@@@@@@@@@@@@ My rank(%d) role (%d) [%d:coord][%d:worker][%d:sched]\n", 
	       rank, mrole, mrole_coordinator, mrole_worker, mrole_scheduler); 

    m_mrole = mrole;

    return mrole;
  }

  mach_role find_role(int mpi_size, int dstnode){

    mach_role mrole = mrole_unknown;
    int sched_machines = m_sp->m_schedulers;      
    int first_schedmach = mpi_size - sched_machines - 1;
    int first_coordinatormach = mpi_size - 1; 

    m_sched_machines = m_sp->m_schedulers;      
    m_first_schedmach = mpi_size - m_sched_machines - 1;
    m_worker_machines = m_first_schedmach;
    m_first_workermach = 0;
    m_first_coordinatormach = mpi_size - 1; 

    if(dstnode == first_coordinatormach){
      mrole = mrole_coordinator; 
    }else if(dstnode < first_schedmach){       
      mrole = mrole_worker;
    }else if(dstnode >= first_schedmach && dstnode < first_coordinatormach){
      mrole = mrole_scheduler;
    }
    assert(mrole != mrole_unknown);

    return mrole;
  }


  void prepare_machine(int mpi_size){
    assert(rank < mpi_size); // sanity check
    m_mpi_size = mpi_size;

#if defined(SVM_COL_PART)
    //    m_weights = (double *)calloc(m_params->m_up->m_samples, sizeof(double));
    //    m_weights_size = m_params->m_up->m_samples;
#endif 

    if(!m_sp->m_scheduling){
      assert(m_sp->m_schedulers == 0);
    }else{
      assert(m_sp->m_schedulers > 0);
    }

    /* assign roles to machines 
     *   first n machines are workers 
     *   next m machines are schedulers if any. this could be zero if scheduling is disabled. 
     *   last one is coordinator
     *  n + m + 1 == mpi_size 
     */
    m_sched_machines = m_sp->m_schedulers;      
    m_first_schedmach = mpi_size - m_sched_machines - 1;
    m_worker_machines = m_first_schedmach;
    m_first_workermach = 0;
    m_first_coordinatormach = mpi_size - 1; 

    if(rank == m_first_coordinatormach){
      m_mrole = mrole_coordinator; 
      //      machine_func = &coordinator_mach;
      m_coordinator_mid = rank - m_first_coordinatormach; 

      strads_msg(ERR, "[prepare_machine] rank(%d) coordinator\n", rank);
      // TODO : fill out scheduler/worekr - send/recvportmap 
    }else if(rank < m_first_schedmach){       
      m_mrole = mrole_worker;
      //      machine_func = &worker_mach;
      m_worker_mid = rank - m_first_workermach; 
      strads_msg(ERR, "[prepare_machine] rank(%d) worker\n", rank);
      // TODO : fill out coordinator send/recvportmap 
    }else if(rank >= m_first_schedmach && rank < m_first_coordinatormach){
      m_mrole = mrole_scheduler;
      //      machine_func = &scheduler_mach;
      m_scheduler_mid = rank - m_first_schedmach; 
      strads_msg(ERR, "[prepare_machine] rank(%d) scheduler\n", rank);
      // TODO : fill out coordinator send/recvportmap 
    }
    assert(m_mrole != mrole_unknown);

    if(m_mrole == mrole_coordinator){ // only coordinator need to make a log for beta, progress log, meta log files 
      assert(m_sp != NULL);
      //      int64_t iterations = m_sp->m_iter;
      //      int64_t progressfreq = m_sp->m_progressfreq;
      //      int64_t logs = iterations / progressfreq;
    }
  }

  void start_framework(void){
    pthread_attr_t attr;
    pthread_t pthid;
    int rc = pthread_attr_init(&attr);
    checkResults("pthread attr init failed\n", rc);
    rc = pthread_create(&pthid, &attr, machine_func, (void *)this);
    checkResults("pthread_create failed\n", rc);
    void *res;
    pthread_join(pthid, &res);
  }

  int get_mpi_size(void){ return m_mpi_size; }

  // data ring stuff
  pthread_mutex_t m_sport_lock; // for flag, not for port
  pthread_mutex_t m_rport_lock; // for flag, not for port
  bool m_sport_flag;
  bool m_rport_flag;
  class _ringport *ring_sport; // once initialized, no more change
  class _ringport *ring_rport; // once initialized, no more change

  // fast ring stuff
  pthread_mutex_t m_fsport_lock; // for flag, not for port
  pthread_mutex_t m_frport_lock; // for flag, not for port
  bool m_fsport_flag;
  bool m_frport_flag;
  class _ringport *ring_fsport; // once initialized, no more change
  class _ringport *ring_frport; // once initialized, no more change

  // star topology stuff -- can be used for any other topology as well. 
  pthread_mutex_t m_starsport_lock; // for flag, not for port
  pthread_mutex_t m_starrport_lock; // for flag, not for port
  bool m_starsport_flag;
  bool m_starrport_flag;
  pthread_mutex_t m_starsend; // for flag, not for port
  pthread_mutex_t m_starrecv; // for flag, not for port
  std::map<uint16_t, _ringport *> star_recvportmap;
  std::map<uint16_t, _ringport *> star_sendportmap;

  sysparam *m_sp;

  int m_scheduler_mid; // scheduler id from 0 to sizeof(schedulers)-1
  int m_worker_mid;    // worker id from 0 to sizeof(workers)-1
  int m_coordinator_mid; // coordinator id from 0 to sizeof(coordinator)-1

  task_assignment m_tmap;

  int m_sched_machines;
  int m_first_schedmach;
  int m_worker_machines;
  int m_first_workermach;
  int m_first_coordinatormach; // first one and last one since only one coordinator now
  int m_mpi_size;

  zmq::context_t *m_zmqcontext;

  // rank number - ringport map 
  std::map<int, _ringport *> scheduler_sendportmap; // coordinator
  std::map<int, _ringport *> scheduler_recvportmap; // coordinator 
  
  std::map<int, _ringport *> coordinator_sendportmap; // workers and scheduler 
  std::map<int, _ringport *> coordinator_recvportmap; // workers and scheduler 

  // star topology only: ring topology does not use this map 
  std::map<int, _ringport *> worker_sendportmap; // coordinator
  std::map<int, _ringport *> worker_recvportmap; // coordinator 


  std::map<int, _ringport *> ring_sendportmap; // coordinator
  std::map<int, _ringport *> ring_recvportmap; // coordinator 


  idval_pair *m_idvalp_buf;
  
  void register_shard(dshardctx *ds){
    std::string *alias = new std::string(ds->m_alias);
    //    strads_msg(ERR, "MALIAS : %c\n", ds->m_alias[0]);
    //    std::string alias("aa");
    std::cout<< "-- SHARD ALIAS " << alias << std::endl;
    m_shardmap.insert(std::pair<std::string, dshardctx *>(*alias, ds));
  }

  dshardctx *get_dshard_with_alias(std::string &alias){
    dshardctx *ret = NULL;
    auto p = m_shardmap.find(alias);
    if(p == m_shardmap.end()){
      ret=NULL;
    }else{
      ret = p->second;
    }
    return ret;    
  }

  std::map<std::string, dshardctx *>m_shardmap; // alias and dshardctx mapping
  std::map<int64_t, staleinfo *>m_stale;

  int64_t get_freethrdscnt(void){
    int64_t ret;
    pthread_mutex_lock(&m_freethrds_lock);
    ret = m_freethrds;
    pthread_mutex_unlock(&m_freethrds_lock);
    return ret;
  }

  pthread_mutex_t m_freethrds_lock;
  int64_t m_freethrds;

#if 0 
  mbuffer **make_msgcollection(void){
    //assert(m_ready_msgcollection);
    m_ready_msgcollection = false;
    for(int i=0; i < m_worker_machines; i++){
      while(1){
	void *buf = worker_recvportmap[i]->ctx->pull_entry_inq();
	if(buf != NULL){
          m_mbuffers[i] = (mbuffer *)buf;
          break;
        }
      }
    }
    return m_mbuffers;
  }
  void release_msgcollection(mbuffer **bufs){
    assert(!m_ready_msgcollection);
    for(int i=0; i < m_worker_machines; i++){    
      worker_recvportmap[i]->ctx->release_buffer((void *)bufs[i]);
    }
    m_ready_msgcollection = true;
  }
#endif 

  scheduler_machctx *m_schedctx;
  coordinator_machctx *m_coordctx;
  worker_machctx *m_workerctx;

  void fork_machagent(void){

    assert(m_mrole != mrole_unknown);
    mach_role role = m_mrole;
    switch(role){
    case mrole_coordinator:

      strads_msg(ERR, " Coordinator call fork agent %d rank  \n", rank);
      m_coordctx = new coordinator_machctx(this);
      break;

    case mrole_scheduler:

      strads_msg(ERR, " Scheduler call fork agent %d rank  \n", rank);
      m_schedctx = new scheduler_machctx(this);
      break;

    case mrole_worker:

      strads_msg(ERR, " Worker call fork agent ... %d rank  \n", rank);
      m_workerctx = new worker_machctx(this);
      break;

    default: 
      assert(0);
    }
  }

  // ring port  send count for preventing overflow 

  //  int64_t m_ringtoken;

  // blocked sync send
  void send(void *buffer, long len){ // workers/schedulers send msg to the coordinator
    assert(m_mrole != mrole_coordinator);       
    star_sendportmap[0]->ctx->push_entry_outq((void *)buffer, len);
  }

  void send(void *buffer, long len, dsttype ws, int wsrank){ // workers/schedulers send msg to the coordinator
    assert(m_mrole == mrole_coordinator); // worker and scheduler are not allowed to use this interface 
    if(ws == dst_scheduler){
      scheduler_sendportmap[wsrank]->ctx->push_entry_outq((void *)buffer, len);
    }else if(ws == dst_worker){
      worker_sendportmap[wsrank]->ctx->push_entry_outq((void *)buffer, len);
    }else{
      assert(0); // unidentified destination type 
    }   
  }

  // asynchronous receiver 
  void *async_recv(void){
    void *ret = NULL;
    assert(m_mrole != mrole_coordinator);       
    ret = star_recvportmap[0]->ctx->pull_entry_inq();
    return ret;
  }
  void *sync_recv(void){
    void *ret = NULL;
    assert(m_mrole != mrole_coordinator);       
    while(ret == NULL){
      ret = star_recvportmap[0]->ctx->pull_entry_inq();
    }
    return ret;
  }


  // asynchronous receiver 
  void *async_recv(int *rlen){
    void *ret = NULL;
    int length=-1;
    assert(m_mrole != mrole_coordinator);       
    ret = star_recvportmap[0]->ctx->pull_entry_inq(&length);
    *rlen = length;
    return ret;
  }
  void *sync_recv(int *rlen){
    int length=-1;
    void *ret = NULL;
    assert(m_mrole != mrole_coordinator);       
    while(ret == NULL){
      ret = star_recvportmap[0]->ctx->pull_entry_inq(&length);     
    }
    *rlen = length;
    return ret;
  }

  void *async_recv(srctype ws, int wsrank){ // workers/schedulers send msg to the coordinator
    void *ret = NULL;
    assert(m_mrole == mrole_coordinator); // worker and scheduler are not allowed to use this interface 
    if(ws == src_scheduler){
      ret = scheduler_recvportmap[wsrank]->ctx->pull_entry_inq();
    }else if(ws == src_worker){
      ret = worker_recvportmap[wsrank]->ctx->pull_entry_inq();
    }else{
      assert(0); // unidentified destination type 
    }   
    return ret;
  }
  void *sync_recv(srctype ws, int wsrank){ // workers/schedulers send msg to the coordinator
    void *ret = NULL;
    assert(m_mrole == mrole_coordinator); // worker and scheduler are not allowed to use this interface 
    if(ws == src_scheduler){
      while(ret == NULL){
	ret = scheduler_recvportmap[wsrank]->ctx->pull_entry_inq();
      }
    }else if(ws == src_worker){
      while(ret == NULL){
	ret = worker_recvportmap[wsrank]->ctx->pull_entry_inq();
      }
    }else{
      assert(0); // unidentified destination type 
    }   
    return ret;
  }

  void *async_recv(srctype ws, int wsrank, int *rlen){ // workers/schedulers send msg to the coordinator

    int length = -1;
    void *ret = NULL;
    assert(m_mrole == mrole_coordinator); // worker and scheduler are not allowed to use this interface 
    if(ws == src_scheduler){
      ret = scheduler_recvportmap[wsrank]->ctx->pull_entry_inq(&length);
    }else if(ws == src_worker){
      ret = worker_recvportmap[wsrank]->ctx->pull_entry_inq();
    }else{
      assert(0); // unidentified destination type 
    }   
    *rlen= length;
    return ret;
  }

  void *sync_recv(srctype ws, int wsrank, int *rlen){ // workers/schedulers send msg to the coordinator
    void *ret = NULL;
    int length=-1;;
    assert(m_mrole == mrole_coordinator); // worker and scheduler are not allowed to use this interface 
    if(ws == src_scheduler){
      while(ret == NULL){
	ret = scheduler_recvportmap[wsrank]->ctx->pull_entry_inq(&length);
      }
    }else if(ws == src_worker){
      while(ret == NULL){
	ret = worker_recvportmap[wsrank]->ctx->pull_entry_inq(&length);
      }
    }else{
      assert(0); // unidentified destination type 
    }   
    *rlen = length;
    return ret;
  }

  pthread_mutex_t m_ringtoken_send_lock;
  pthread_mutex_t m_ringtoken_recv_lock;

  int64_t m_ringtoken_send;
  int64_t m_ringtoken_recv;

  void *ring_asyncrecv_aux(void){
    assert(0);
    void *ret = NULL;
    pthread_mutex_lock(&m_ringtoken_recv_lock);
    assert(m_ringtoken_recv <= IHWM);
    if(m_mrole == mrole_coordinator){
      ret = ring_recvportmap[rdataport]->ctx->ring_pull_entry_inq();
    }else if(m_mrole == mrole_worker){
      ret = ring_recvportmap[rdataport]->ctx->ring_pull_entry_inq();
    }else{
      strads_msg(ERR, "Scheduler is not supposed to user RING API yet\n");
      assert(0);
    }
  
    if(ret != NULL){
      m_ringtoken_recv--;
      assert(m_ringtoken_recv >= 0);
      if(m_ringtoken_recv == 0){
	if(m_mrole == mrole_coordinator){
	  void *tmpbuf = (void *)calloc(1024, sizeof(char));
	  sprintf((char *)tmpbuf, "TOKEN from receiver to sender");
	  ring_sendportmap[rackport]->ctx->ring_push_entry_outq((void *)tmpbuf, 1024);
	  m_ringtoken_recv = IHWM;
	}else if(m_mrole == mrole_worker){
	  void *tmpbuf = (void *)calloc(1024, sizeof(char));
	  sprintf((char *)tmpbuf, "###### TOKEN from receiver (%d) to sender", rank);
	  ring_sendportmap[rackport]->ctx->ring_push_entry_outq((void *)tmpbuf, 1024);
	  m_ringtoken_recv = IHWM;
	}else{
	  strads_msg(ERR, "Scheduler is not supposed to user RING API yet\n");
	  assert(0);
	}
      }      
    }
    pthread_mutex_unlock(&m_ringtoken_recv_lock);
    return ret;
  } // ring_asyncrecv 

  void ring_send_aux(void * buffer, long blen){
    assert(0);

    pthread_mutex_lock(&m_ringtoken_send_lock);
    if(m_ringtoken_send == 0){
      if(m_mrole == mrole_coordinator){
	while(1){
	  void *recv = NULL;
	  while(recv == NULL){
	    recv = ring_recvportmap[rackport]->ctx->ring_pull_entry_inq();
	  }
	  m_ringtoken_send = IHWM;
	  break;
	}
      }else if(m_mrole == mrole_worker){
	while(1){
	  void *recv = NULL;
	  while(recv == NULL){
	    recv = ring_recvportmap[rackport]->ctx->ring_pull_entry_inq();
	  }
	  m_ringtoken_send = IHWM;
	  break;
	}
      }
    }
    if(m_mrole == mrole_coordinator){
      ring_sendportmap[rdataport]->ctx->ring_push_entry_outq((void *)buffer, blen);
    }else if(m_mrole == mrole_worker){
      ring_sendportmap[rdataport]->ctx->ring_push_entry_outq((void *)buffer, blen);
    }else{
      strads_msg(ERR, "Scheduler is not supposed to user RING API yet\n");
      assert(0);
    }
    m_ringtoken_send--;
    assert(m_ringtoken_send >= 0);
    pthread_mutex_unlock(&m_ringtoken_send_lock);
  } // ring_asyncrecv 

  void *ring_async_send_aux(void * buffer, long blen){
    assert(buffer != NULL);
    assert(m_mrole != mrole_scheduler);
    pthread_mutex_lock(&m_ringtoken_send_lock);
    if(m_ringtoken_send == 0){
      while(1){
	void *recv = NULL;      
	recv = ring_recvportmap[rackport]->ctx->ring_pull_entry_inq();	
	if(recv != NULL){
	  m_ringtoken_send = IHWM;	    
	  break;
	}else{
	  pthread_mutex_unlock(&m_ringtoken_send_lock);
	  return NULL;
	}
      }
    }
    ring_sendportmap[rdataport]->ctx->ring_push_entry_outq((void *)buffer, blen);
    m_ringtoken_send--;
    assert(m_ringtoken_send >= 0);
    pthread_mutex_unlock(&m_ringtoken_send_lock);
    return buffer;
  } // ring_asyncrecv 

  void *ring_asyncrecv_aux(int *rlen){
    assert(m_mrole != mrole_scheduler);
    void *ret = NULL;
    pthread_mutex_lock(&m_ringtoken_recv_lock);
    assert(m_ringtoken_recv <= IHWM);
    ret = ring_recvportmap[rdataport]->ctx->ring_pull_entry_inq(rlen);
    if(ret != NULL){
      m_ringtoken_recv++;
      assert(m_ringtoken_recv <= IHWM);
      if(m_ringtoken_recv == IHWM){	
	void *tmpbuf = (void *)calloc(32, sizeof(char));
	sprintf((char *)tmpbuf, "TOKEN");
	ring_sendportmap[rackport]->ctx->ring_push_entry_outq((void *)tmpbuf, 32);
	m_ringtoken_recv = 0;
      }
    }      
    pthread_mutex_unlock(&m_ringtoken_recv_lock);
    return ret;
  } // ring_asyncrecv 

}; // sharectx 
























#if 0 

class ngraphctx {
public:
  std::map<int, stnode *> nodes; // mapping between MPI rank and stnodes 
}; // node graph context 

class parameters{
public:

  parameters(sysparam *sp)
    : m_sp(sp),  m_avail(true){
  }

  parameters()
    : m_sp(NULL),  m_avail(false){
  }
  ~parameters(){   
  }
  void print_allparams(void){
    //    strads_msg(INF, "---------------  user    parameters ----------------- \n");
    //    m_up->print();
    //    strads_msg(INF, "---------------  system  parameters ----------------- \n");
    //    m_sp->print();
    //    strads_msg(INF, "---------------  machine parameters ----------------- \n");
    //    m_machspec->print();
    strads_msg(INF, "---------------  End of  parameters ----------------- \n");
  }
  sysparam *m_sp;
private:
  bool m_avail;
};

#define CMDSTATE_PENDING (0x10)
#define CMDSTATE_DONE (0x20)
#define CMDSTATE_NOT_ASSIGNED (0x0)
// command was delivered to worker(s), and done ack arrived

enum cmdstatus { CMD_UNCOMPLETE=100, CMD_COMPLETE=200 };

typedef struct{
  long cmdid;
  machtype mtype;
  int receivers;
  int base;
  char machines[MAX_MACHINES];  
  int pendingmach;
  cmdstatus status; 
}cmdentry;

// pendingpacket management 
class ppacketmgt{

public:
  ppacketmgt(int maxsysc, int maxuserc, int firstsched, int schedmach, int firstworker, int workermach)
    : m_max_scmd(maxsysc), m_max_ucmd(maxuserc), m_firstsched(firstsched), m_schedmach(schedmach), 
      m_firstworker(firstworker), m_workermach(workermach), m_cmdclock(0){
    m_syscmdq_lock = PTHREAD_MUTEX_INITIALIZER;
    m_usercmdq_lock = PTHREAD_MUTEX_INITIALIZER;
  }

  ppacketmgt(int maxsysc, int maxuserc, class sharedctx &ctx)
    : m_max_scmd(maxsysc), m_max_ucmd(maxuserc), m_firstsched(ctx.m_first_schedmach), 
      m_schedmach(ctx.m_sched_machines), m_firstworker(ctx.m_first_workermach), 
      m_workermach(ctx.m_worker_machines), m_cmdclock(0){
    m_syscmdq_lock = PTHREAD_MUTEX_INITIALIZER;
    m_usercmdq_lock = PTHREAD_MUTEX_INITIALIZER;
  }

  void push_cmdq(long cmdid, machtype mtype){    
    cmdentry *entry = (cmdentry *)calloc(1, sizeof(cmdentry));
    // CAVEAT: entry should be initialized to zero since NOT_ASSIGNED status is defined as 0x0
    int base=-1, receivers=-1;
    entry->cmdid = cmdid;    
    if(mtype == m_worker){
      base = m_firstworker;
      receivers = m_workermach;     
    }else if(mtype == m_scheduler){
      base = m_firstsched;
      receivers = m_schedmach;     
    }   
    assert(base>=0);
    assert(receivers>0);

    entry->receivers = receivers;
    entry->base = base;

    for(int i=0; i < receivers; i++){
      entry->machines[base + i]=CMDSTATE_PENDING;
    }
    entry->status = CMD_UNCOMPLETE;
    entry->pendingmach = receivers;
    entry->mtype = mtype;

    int rc = pthread_mutex_lock(&m_syscmdq_lock);
    checkResults("[push cmdq] get lock failed", rc);
    m_syscmdpendq.insert(std::pair<long, cmdentry *>(cmdid, entry));
    rc = pthread_mutex_unlock(&m_syscmdq_lock);
    checkResults("[push cmdq] release lock failed", rc);
  }

  long mark_oneack(mbuffer *mbuf){
    long cmdid = mbuf->cmdid;    
    long src = mbuf->src_rank; // rank number

    int rc = pthread_mutex_lock(&m_syscmdq_lock);
    checkResults("[push mark_oneack] get lock failed", rc);
    std::unordered_map<long, cmdentry *>::iterator it = m_syscmdpendq.find(cmdid);
    if(it == m_syscmdpendq.end()){
      LOG(FATAL) << "Fatal: cmdid " << cmdid << " is not found in the pending command q" << std::endl;
    }
    cmdentry *entry = it->second;

    strads_msg(ERR, "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ cmdid(%ld) src(%ld)\n", cmdid, src);

    assert(entry->machines[src] == CMDSTATE_PENDING);

    entry->machines[src] = CMDSTATE_DONE;
    entry->pendingmach--; 
    assert(entry->pendingmach >= 0);    

    if(entry->pendingmach == 0){
      entry->status = CMD_COMPLETE;
      m_syscmdpendq.erase(cmdid);
      m_syscmddoneq.insert(std::pair<long, cmdentry *>(cmdid, entry));     
    }

    rc = pthread_mutex_unlock(&m_syscmdq_lock);
    checkResults("[push mark_oneack] release lock failed", rc);   
    return cmdid;
  }

  // free entry that was done and checked here 
  bool check_cmddone(long cmdid){
    bool ret;
    int rc = pthread_mutex_lock(&m_syscmdq_lock);
    checkResults("[push mark_oneack] get lock failed", rc);
    std::unordered_map<long, cmdentry *>::iterator it = m_syscmddoneq.find(cmdid);    
    if(it != m_syscmddoneq.end()){
      ret = true;
      m_syscmddoneq.erase(cmdid);
      free(it->second); 
      // this is matching with calloc above 
      // if replaced with pool, this wil be replaced with release pool entry 
    }else{
      ret = false;
    }
    rc = pthread_mutex_unlock(&m_syscmdq_lock);
    checkResults("[push mark_oneack] get lock failed", rc);
    return ret;
  }


  long get_cmdclock(void){
    int rc = pthread_mutex_lock(&m_syscmdq_lock);
    checkResults("[push mark_oneack] get lock failed", rc);
    long cmdclock = m_cmdclock;
    m_cmdclock++; // for next call 
    rc = pthread_mutex_unlock(&m_syscmdq_lock);
    checkResults("[push mark_oneack] get lock failed", rc);
    return cmdclock;
  }

  void print_syscmdpendq(void){
    strads_msg(ERR, "[@@@@@@@ SYS_PENDQ] size(%ld)\n", m_syscmdpendq.size());     

    int rc = pthread_mutex_lock(&m_syscmdq_lock);
    checkResults("[print syscmd pendq ] get lock failed", rc);
    for(auto p: m_syscmdpendq){
      long id = p.first;
      cmdentry *entry = p.second;
      assert(entry->cmdid == id);           
      strads_msg(ERR, "[@@@@@@@ SYS_PENDQ] cmdid(%ld) mtype(%d) recv(%d) base(%d) pmach(%d) cmdstate(%d)\n", 
		 entry->cmdid, entry->mtype, entry->receivers, entry->base,entry->pendingmach, entry->status);      
      for(int i=0; i < entry->receivers; i++){
	strads_msg(ERR, " pendq_mach[%d]=(%d) ", i+entry->base, entry->machines[i+entry->base]); 
      }
    }
    rc = pthread_mutex_unlock(&m_syscmdq_lock);
    checkResults("[print syscmd pendq ] release lock failed", rc);
  }

  void print_syscmddoneq(void){
    strads_msg(ERR, "[@@@@@@@ SYS_DONEQ] size(%ld)\n", m_syscmddoneq.size());     
    int rc = pthread_mutex_lock(&m_syscmdq_lock);
    checkResults("[print syscmd doneq ] get lock failed", rc);
    for(auto p: m_syscmddoneq){
      long id = p.first;
      cmdentry *entry = p.second;
      assert(entry->cmdid == id);           
      strads_msg(ERR, "[@@@@@@@ SYS_DONEQ] cmdid(%ld) mtype(%d) recv(%d) base(%d) pmach(%d) cmdstate(%d)\n", 
		 entry->cmdid, entry->mtype, entry->receivers, entry->base,entry->pendingmach, entry->status);      
      for(int i=0; i < entry->receivers; i++){
	strads_msg(ERR, " doneq_mach[%d]=(%d) ", i+entry->base, entry->machines[i+entry->base]); 
      }
    }
    rc = pthread_mutex_unlock(&m_syscmdq_lock);
    checkResults("[print syscmd doneq] release lock failed", rc);
  }

  uint64_t get_syscmddoneq_size(void){
    uint64_t size;
    int rc = pthread_mutex_lock(&m_syscmdq_lock);
    checkResults("[get syscmd doneq size ] get lock failed", rc);
    size = m_syscmddoneq.size();
    rc = pthread_mutex_unlock(&m_syscmdq_lock);
    checkResults("[get syscmd doneq size ] release lock failed", rc);
    return size;
  }

  uint64_t get_syscmdpendq_size(void){
    uint64_t size;
    int rc = pthread_mutex_lock(&m_syscmdq_lock);
    checkResults("[get syscmd doneq size ] get lock failed", rc);
    size = m_syscmdpendq.size();
    rc = pthread_mutex_unlock(&m_syscmdq_lock);
    checkResults("[get syscmd doneq size ] release lock failed", rc);
    return size;
  }
  
private:
  int m_max_scmd; // max limits on the number of pending system command
  int m_max_ucmd; // max limits on the number of pending user command 
    std::unordered_map<long, cmdentry *>m_syscmdpendq;
  std::unordered_map<long, cmdentry *>m_syscmddoneq;
  std::unordered_map<long, cmdentry *>m_usercmdpendq;
  std::unordered_map<long, cmdentry *>m_usercmddoneq;
  int m_firstsched;
  int m_schedmach;
  int m_firstworker;
  int m_workermach;
  long m_cmdclock; // command clock 
  pthread_mutex_t m_syscmdq_lock;
  pthread_mutex_t m_usercmdq_lock;
  // TODO : think about circular scheme 
};
#endif 










































//////////////////////////////////// OBSOLETE CODE /////////////////////////////////////////////////////////////
#if 0 
#define NO_TIMING_LOG
typedef struct{ 
  int events;
  uint64_t event_start[MAX_EVENT_PER_CMD]; // micro second
  uint64_t event_end[MAX_EVENT_PER_CMD];   // micro second  
}commandlog;
class timing_log{
public:
  timing_log(int64_t maxcommand, int mid, int lid, int gid, const char *tlogprefix)
  : m_maxcommand(maxcommand), m_mid(mid), m_lid(lid), m_gid(gid){
#if !defined(NO_TIMING_LOG)
    m_cmdlog = (commandlog *)calloc(sizeof(commandlog), maxcommand);      
    for(int64_t i=0; i<maxcommand; i++){
      m_cmdlog[i].events = -1;
    }
    m_logfn = (char *)calloc(strlen(tlogprefix)+20, sizeof(char));
    strcpy(m_logfn, tlogprefix);
    char tmp[10];
    sprintf(tmp, "%d.timelog", mid);
    strcat(m_logfn, tmp);
    strads_msg(ERR, "@@@@@@@@@ TIMING LOG file name : %s \n", 
	       m_logfn);
    m_fd = (FILE *)fopen(m_logfn, "wt");
    assert(m_fd);
#endif 
  }
  timing_log(void){}  
  void write_cmdevent_start_log(int64_t cmdid, int eventid, uint64_t start){
#if !defined(NO_TIMING_LOG)
    assert(cmdid <= m_maxcommand);
    strads_msg(ERR, "@@@@@ write event start log cmd id %ld   %d   %ld\n", cmdid, eventid, start); 
    m_cmdlog[cmdid].event_start[eventid] = start;
    if(m_cmdlog[cmdid].events < eventid){
      m_cmdlog[cmdid].events = eventid;
    }	
#endif 
  }
  void write_cmdevent_end_log(int64_t cmdid, int eventid, uint64_t end){
#if !defined(NO_TIMING_LOG)
    assert(cmdid <= m_maxcommand);
    strads_msg(ERR, "@@@@@ write event end log %ld %d %ld\n", cmdid, eventid, end); 
    m_cmdlog[cmdid].event_end[eventid] = end;
    //    assert(m_cmdlog[cmdid].events == eventid);     
#endif 
  }
  void flush_hdd(void){
#if !defined(NO_TIMING_LOG)
    uint64_t rbase = 1397119834728351;
    for(int64_t i=0; i < m_maxcommand; i++){
      if(m_cmdlog[i].events >=0){
	for(int j=0; j<m_cmdlog[i].events+1; j++){
	  if(m_cmdlog[i].event_start[j] == 0)
	    continue;
	  fprintf(m_fd, "set object  %ld rect from  %ld, %ld to %ld, %ld\n", 		  
		  //	  i, m_cmdlog[i].event_start[j] -rbase, i*10, m_cmdlog[i].event_end[j]-rbase, i*10 + 5);
		  i*50 + j, m_cmdlog[i].event_start[j]-rbase, i*50 + j, m_cmdlog[i].event_end[j]-rbase, i*50 + j + 1);
	  int idx = i*50 + j + 1;
	  int color;
	  if(idx%50 == 0){
	    color=8;
	  }else if(idx % 20 == 0){
	    color = 9;
	  }else{	   
	    color = idx%10;
	  }
	  fprintf(m_fd, "set object  %ld front lw 1.0 fc  lt %d fillstyle solid 1.00 border lt -1\n", i*50 + j, color);	    
	}
      }
    }
#endif 
  }
private:
  int64_t m_maxcommand;
  commandlog *m_cmdlog;
  int m_mid;
  int m_lid;
  int m_gid;
  char *m_logfn;
  FILE *m_fd;
};
typedef struct{
  int64_t iteration1;
  int64_t iteration2;
  uint64_t elapsedtime;
  double object;
}logentry;
typedef struct{
  uint64_t stime;
  uint64_t etime;
}timelog;
class timers{
public:
  timers(int timercnt, int eventcnt): m_timer_cnt(timercnt), m_automode(false), m_event_cnt(eventcnt){
    m_logs = (timelog *)calloc(m_timer_cnt, sizeof(timelog));
    memset(m_logs, 0, m_timer_cnt*sizeof(timelog));
    m_free_entry = 0;
    m_elapsedtime = (uint64_t *)calloc(m_event_cnt, sizeof(uint64_t));  // it should be calloc 
    memset(m_elapsedtime, 0, m_event_cnt*sizeof(uint64_t));
  }
  ~timers(){
    free(m_logs);
  }
  uint64_t set_stimer(int idx){
    assert(m_automode == false);
    m_logs[idx].stime = timenow();
    return m_logs[idx].stime;
  }
  uint64_t set_etimer(int idx){
    assert(m_automode == false);
    m_logs[idx].etime = timenow();
    return m_logs[idx].etime;
  }
  uint64_t get_elapsedtime(int idx){ return (m_logs[idx].etime - m_logs[idx].stime); }
  uint64_t set_auto_stimer(void){
    assert(m_automode == true);
    m_logs[m_free_entry].stime = timenow();
    return m_logs[m_free_entry].stime;
  }
  uint64_t set_auto_etimer(void){
    assert(m_automode == true);
    m_logs[m_free_entry].etime = timenow();
    uint64_t ret = m_logs[m_free_entry].etime;
    m_free_entry ++;
    return ret;
  }
  void update_elapsedtime(int start, int end){
    assert(start >= 0 && start <= m_event_cnt-1);
    assert(end >= 0 && end <= m_event_cnt-1);
    assert(end >= start);
    for(int i=start; i <= end; i++){
      m_elapsedtime[i] += (m_logs[i].etime - m_logs[i].stime); 
    }
  }
  void print_elapsedtime(int start, int end){
    assert(start >= 0 && start <= m_event_cnt-1);
    assert(end >= 0 && end <= m_event_cnt-1);
    assert(end >= start);
    for(int i=start; i <= end; i++){
      strads_msg(INF, "\t\tEvent [%d] elapsed time: %lf milli-second \n", 
		 i, m_elapsedtime[i] / 1000.0); 
      m_elapsedtime[i] = 0;
    }    
  }
  void print_elapsedtime(int start, int end, int id){
    assert(start >= 0 && start <= m_event_cnt-1);
    assert(end >= 0 && end <= m_event_cnt-1);
    assert(end >= start);
    for(int i=start; i <= end; i++){
      strads_msg(INF, "\t\t\t\tThid(%d) Event [%d] elapsed time: %lf milli-second \n", 
		 id, i, m_elapsedtime[i] / 1000.0); 
      m_elapsedtime[i] = 0;
    }    
  }  
private:
  int m_timer_cnt;
  timelog *m_logs;
  int m_free_entry;
  const bool m_automode;
  uint64_t *m_elapsedtime;
  int m_event_cnt;
};
#endif 
